# TechSharing
## Unlock Spark from A to Z
“Big data" analysis is a hot and highly valuable skill – and this series will discuss with you the hottest technology in big data: Apache Spark. More and more teams are using Spark to quickly extract meaning from massive data sets across a fault-tolerant Hadoop cluster.

What Shall We Discuss together?

- Frame big data analysis problems as Apache Spark scripts
- Develop distributed code using the Scala programming language 
- Optimize Spark jobs through partitioning, caching, and other techniques
- Build, deploy, and run Spark scripts on EMR clusters
- Process continual streams of data with Spark Streaming
- Transform structured data using SparkSQL and DataFrames
- It maybe the first time to know these topics, but you really don't need to prepare too much, it's easier than you might think.

Learn and master the art of framing data analysis problems as Spark problems through over 20 hands-on examples, and then scale them up to run on cloud computing services in this course.

- Learn the concepts of Spark's Resilient Distributed Datastores
- Get a crash course in the Scala programming language
- Develop and run Spark jobs quickly using Scala
- Translate complex analysis problems into iterative or multi-stage Spark scripts
- Scale up to larger data sets using Amazon's Elastic MapReduce service
- Understand how Hadoop YARN distributes Spark across computing clusters
- Practice using other Spark technologies, like Spark SQL, DataFrames, DataSets, Spark Streaming
- By the end of this series, we'll be running code that analyzes gigabytes worth of information – in the cloud – in a matter of minutes. 

We'll have some fun along the way. You'll get warmed up with some simple examples of using Spark to analyze movie ratings data and text in a book. Once you've got the basics under your belt, we'll move to some more complex and interesting tasks. We'll use a million movie ratings to find movies that are similar to each other, and you might even discover some new movies you might like in the process!

This series is very hands-on; you'll spend most of your time following along with the instructor as we write, analyze, and run real code together – both on your own system, and in the cloud using Amazon's Elastic MapReduce service. With over 20 real examples of increasing complexity you can build, run and study yourself. Move through them at your own pace, on your own schedule. The series wraps up with an overview of other Spark-based technologies, including Spark SQL, Spark Streaming, and MLLib.

Enjoy it!

You need to know

Some prior programming or scripting experience is required.
A crash course in Scala is included, but you need to know the fundamentals of programming in order to pick it up.

### Sessions



## Mastering Concurrency Programming with Java 8
  
Mastering the principles and techniques of multithreaded programming with the Java 8 Concurrency API.

it will cover below topics:

- Concurrency Design Principles
- Managing Lots of Threads – Executors
- Getting the Maximum from Executors
- Getting Data from the Tasks – The Callable and Future Interfaces
- Running Tasks Divided into Phases – The Phaser class
- Optimizing Divide and Conquer Solutions – The Fork/Join Framework
- Processing Massive Datasets with Parallel Streams – The Map and Reduce Model
- Processing Massive Datasets with Parallel Streams – The Map and Collect
- Diving into Concurrent Data Structures and Synchronization Utilities
- Testing and Monitoring Concurrent Applications
- I split above topics into 4 parts:


### Sessions
- Part 1: Concurrency Design Principles and Executors
- Part 2: Tasks and Fork/Join
- Part 3: Parallel Streams
- Part 4: Concurrent Data Structures, Synchronization Utilities, and Testing
- Part 5: Advantage concurrent framework: Disruptor and Akka
